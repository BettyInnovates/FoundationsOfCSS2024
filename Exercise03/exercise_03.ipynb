{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Evaluation of Twitter and YouTube Data\n",
    "## Tasks\n",
    "\n",
    "1. Install packages and load evaluation datasets with Google NLP scores\n",
    "2. Run VADER over evaluation texts\n",
    "3. Run BERT over evaluation texts\n",
    "4. Evaluate against sentiment annotations and compare with Google NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements. \n",
    "\n",
    "The following cell contains all the necessary dependencies needed for this task. If you run the cell everything will be installed. \n",
    "\n",
    "* [`vaderSentiment`](https://github.com/cjhutto/vaderSentiment) is a Python package for a Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text.\n",
    "* [`transformers`](https://huggingface.co/) is a Python package for creating and working with transformers. [Here](https://huggingface.co/docs) is the documentation of `transformers`.\n",
    "* [`torch`](https://pytorch.org/) is a Python machine learning framework. We need this here for `transformers` since this package uses internally `torch`. [Here](https://pytorch.org/docs/stable/index.html) is the documentation of `torch`.\n",
    "* [`pandas`](https://pandas.pydata.org/docs/index.html) is a Python package for creating and working with tabular data. [Here](https://pandas.pydata.org/docs/reference/index.html) is the documentation of `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2023.5.7)\n",
      "Requirement already satisfied: transformers in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.47.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: torch in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: python-certifi-win32 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: wrapt>=1.10.4 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-certifi-win32) (1.15.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-certifi-win32) (2023.5.7)\n",
      "Requirement already satisfied: setuptools-scm in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-certifi-win32) (8.1.0)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from setuptools-scm->python-certifi-win32) (24.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from setuptools-scm->python-certifi-win32) (63.2.0)\n",
      "Requirement already satisfied: tomli>=1 in c:\\users\\mub\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from setuptools-scm->python-certifi-win32) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install vaderSentiment\n",
    "! pip install transformers sentencepiece\n",
    "! pip install torch torchvision torchaudio\n",
    "! pip install pandas\n",
    "# certs for huggingface.co on windows python\n",
    "! pip install python-certifi-win32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to restart the Kernel after installing the dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements\n",
    "The cell below imports all necessary dependancies. Make sure they are installed (see cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load evaluation datasets and Google NLP scores\n",
    "\n",
    "## 1.1 Load datasets\n",
    "First read the Twitter and Youtube Comments CSV files (`Twitter-Sentiment.csv` and `YouTubeComments-Sentiment.csv`) and save them in a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Twitter data\n",
    "twitter_data = pd.read_csv(\"Twitter-Sentiment.csv\")\n",
    "# print(twitter_data)\n",
    "\n",
    "# Read Youtube data\n",
    "youtube_data = pd.read_csv(\"YouTubeComments-Sentiment.csv\")\n",
    "# print(youtube_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run VADER over evaluation texts\n",
    "\n",
    "## 2.1 Run VADER over the first tweet\n",
    "\n",
    "In this task you should use VADER for sentiment analysis. For this we use the `vaderSentiment` package. You first have to intatiate a new `SentimentIntensityAnalyzer` and use the `polarity_scores` method of it for the analysis. Apply this for the first tweet. Is it a good classification?\n",
    "\n",
    "[Here](https://github.com/cjhutto/vaderSentiment) under 'Code Examples' you can find some example code how to use this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Tweet: ?RT @justinbiebcr: The bigger the better....if you know what I mean ;)\n",
      "\n",
      "Classification first Tweet: {'neg': 0.0, 'neu': 0.853, 'pos': 0.147, 'compound': 0.2263}\n",
      "\n",
      "Label of First Tweet: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Intatiate a new SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Cassify first tweet and print\n",
    "first_tweet = twitter_data[\"text\"][0]\n",
    "first_tweet_classification = vader.polarity_scores(first_tweet)\n",
    "first_tweet_label = twitter_data[\"label\"][0]\n",
    "\n",
    "print(f\"First Tweet: {first_tweet}\\n\")\n",
    "print(f\"Classification first Tweet: {first_tweet_classification}\\n\")\n",
    "print(f\"Label of First Tweet: {first_tweet_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzed tweet is predominantly neutral (neu: 0.853) but leans slightly positive overall (compound: 0.2263 and pos: 0.147).\n",
    "There’s no detectable negativity (neg: 0.0), so the tone of the tweet is likely neutral to mildly positive which somehow corresponds to its label.\n",
    "\n",
    "The classification is reasonable but not perfect. VADER captures the neutral structure and mild positivity but misses the playful, suggestive tone implied by the wink emoji and double entendre. It also overlooks the broader context and cultural nuances, such as the implied humor in \"if you know what I mean.\" While suitable for general analysis, it lacks the sophistication to interpret subtle humor, innuendo, or contextual cues in tweets like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run VADER over each text\n",
    "\n",
    "Now use VADER for all the text data of the Twitter and the Youtube dataframe. Create a new column in the dataframes called `VADER_compound` where you save the `compound` result (look at the output dictonary of the `polarity_scores` method).\n",
    "\n",
    "*Important: Make sure `compound` is a float*\n",
    "\n",
    "If this runs slow on your computer you can use the precomputed values in the provided CSV files which are present in the column `VADER_compund_precomputed` for further tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Vader Compound vs Precomputed (Twitter):100.0%\n"
     ]
    }
   ],
   "source": [
    "# Using VADER for sentiment analysis of twitter data\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "twitter_data[\"VADER_compound\"] = 0.0\n",
    "\n",
    "#for i in range(10):\n",
    "for i in range(len(twitter_data[\"text\"])):\n",
    "    # use polarity_scores method to get the sentiment scores\n",
    "    sentiment_dict = vader.polarity_scores(twitter_data[\"text\"][i])\n",
    "    # Save the compound result as float in the dataset. \n",
    "    # Notice: .loc is way slower here.... but worked for us ;)\n",
    "    twitter_data.loc[i, \"VADER_compound\"] = sentiment_dict[\"compound\"]\n",
    "\n",
    "# Test against precomputed values\n",
    "match_VADER_twitter = twitter_data[twitter_data[\"VADER_compound\"] == twitter_data[\"VADER_compound_precomputed\"]].shape[0] \n",
    "print(f\"Match Vader Compound vs Precomputed (Twitter):{(match_VADER_twitter / len(twitter_data.index))* 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Vader Compound vs Precomputed (YouTube):100.0%\n"
     ]
    }
   ],
   "source": [
    "# Using VADER for sentiment analysis of YouTube data\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "youtube_data[\"VADER_compound\"] = 0.0\n",
    "\n",
    "#for i in range(10):\n",
    "for i in range(len(youtube_data[\"text\"])):\n",
    "    # use polarity_scores method to get the sentiment scores\n",
    "    sentiment_dict = vader.polarity_scores(youtube_data[\"text\"][i])\n",
    "    # Save the compound result as float in the dataset. \n",
    "    # Notice: .loc is way slower here.... but worked for us ;)\n",
    "    youtube_data.loc[i, \"VADER_compound\"] = sentiment_dict[\"compound\"]\n",
    "\n",
    "# Test against precomputed values\n",
    "match_VADER_youtube = youtube_data[youtube_data[\"VADER_compound\"] == youtube_data[\"VADER_compound_precomputed\"]].shape[0] \n",
    "print(f\"Match Vader Compound vs Precomputed (YouTube):{(match_VADER_youtube / len(youtube_data.index))* 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 VADER as a classifier\n",
    "\n",
    "To get the three Classes `Positive`, `Negative` and `Neutral` we use the compound score with the following thresholds:\n",
    "\n",
    "* `compound > 0.5`: `\"Positive\"`\n",
    "* `compound < -0.5`: `\"Negative\"`\n",
    "* `else`: `\"Neutral\"`\n",
    "\n",
    "Create a new column called `VADER_class` which contains the three computed classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column for computed classes\n",
    "twitter_data[\"VADER_class\"] = \"Neutral\"\n",
    "youtube_data[\"VADER_class\"] = \"Neutral\"\n",
    "\n",
    "# Classify Twitter Data\n",
    "twitter_data.loc[twitter_data[\"VADER_compound\"] > 0.5, \"VADER_class\"] = \"Positive\"\n",
    "twitter_data.loc[twitter_data[\"VADER_compound\"] < 0.5, \"VADER_class\"] = \"Negative\"\n",
    "\n",
    "# Classify YouTube Data\n",
    "youtube_data.loc[youtube_data[\"VADER_compound\"] > 0.5, \"VADER_class\"] = \"Positive\"\n",
    "youtube_data.loc[youtube_data[\"VADER_compound\"] < 0.5, \"VADER_class\"] = \"Negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use a BERT based model for sentiment analysis\n",
    "\n",
    "## 3.1 BERT\n",
    "BERT (Bidirectional Encoder Representation from Transformers) is a machine learning technique for natural language processing. There are already pretrained models available in the `transformers` package. You can look [here](https://huggingface.co/models?sort=downloads&search=sentiment) and choose a model for the next tasks. (We suggest [this](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) (`\"cardiffnlp/twitter-roberta-base-sentiment-latest\"`) model, but you can use any available, just make sure it is suitable for sentiment analysis).\n",
    "\n",
    "First create a `pipeline` where you set your model by the `model` keyword argument. You can then use this method to pass text which should be classified. [Here](https://huggingface.co/blog/sentiment-analysis-python#2-how-to-use-pre-trained-sentiment-analysis-models-with-python) is a tutorial how to use this.\n",
    "\n",
    "As before save the classes in a new row 'BERT_class'. The call to your pipeline returns a dictionary where there is a key `label` which contains already the `Positive`, `Negative` or `Neutral` class (Be aware that this is based on the model you choose, sometimes these classes are named differently so you have to rename them by hand, this is not the case if you use the suggested model).\n",
    "\n",
    "Based on you computer this may take some time, if it is too slow for you, you can again use the precomputed classes `'BERT_class_precomputed'` in the CSV Files for further tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BERT-Base-Uncased model for sentiment analysis\n",
    "# sentiment_pipeline = pipeline(model=f\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was not possible for us to use any models from HuggingFace for two reasons:\n",
    " - Private Notebooks have no GPU and running on CPU would take for hours\n",
    " - Availabel notebook with GPU (working/company notebook) had problems downloading models from huggingface.co directly due to firewall restrions (http requests are filter with company proxy, missing e-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column for computed BERT classes\n",
    "twitter_data[\"BERT_class\"] = \"Neutral\"\n",
    "youtube_data[\"BERT_class\"] = \"Neutral\"\n",
    "\n",
    "# twitter_data\n",
    "for i in range(10):\n",
    "    # use the sentiment_pipeline to get the sentiment scores\n",
    "    sentiment_dict = sentiment_pipeline(...)\n",
    "    # Save the class result as string in the dataset. Notice: .loc is way slower here....\n",
    "    twitter_data.loc[i, 'BERT_class'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate against sentiment annotations and compare with Google NLP\n",
    "\n",
    "## 4.1 Convert GoogleNLP scores to classes\n",
    "\n",
    "As with VADER and BERT, compute classes from the GoogleNLP score, which is given in the column `googleScore`. For this use following thresholds:\n",
    "\n",
    "* `googleScore > 0.3`: `\"Positive\"`\n",
    "* `googleScore < -0.3`: `\"Negativ\"`\n",
    "* `else`: `\"Neutral\"`\n",
    "\n",
    "Save the classes in a new column named `GoogleNLP_class`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column for Google NLP classes\n",
    "twitter_data[\"GoogleNLP_class\"] = \"Neutral\"\n",
    "youtube_data[\"GoogleNLP_class\"] = \"Neutral\"\n",
    "\n",
    "# Classify Twitter Data\n",
    "twitter_data.loc[twitter_data[\"googleScore\"] > 0.3, \"GoogleNLP_class\"] = \"Positive\"\n",
    "twitter_data.loc[twitter_data[\"googleScore\"] < -0.3, \"GoogleNLP_class\"] = \"Negative\"\n",
    "\n",
    "# Classify YouTube Data\n",
    "youtube_data.loc[youtube_data[\"googleScore\"] > 0.3, \"GoogleNLP_class\"] = \"Positive\"\n",
    "youtube_data.loc[youtube_data[\"googleScore\"] < -0.3, \"GoogleNLP_class\"] = \"Negative\"\n",
    "\n",
    "# print(youtube_data)\n",
    "# print(twitter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate on Twitter\n",
    "\n",
    "First, let's calculate the accuracy for all three classifiers on the Twitter and Youtube data, print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Formula\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Samples}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Calculate Accuracy\n",
    "def calculateAccuracy(dataset: pd.DataFrame, column_name_label: str, column_name_prediction: str) -> float:\n",
    "    \n",
    "    # Get Total number of Samples\n",
    "    Total_Number_of_Samples = len(dataset.index)\n",
    "    print(f\"Total Number of Samples: {Total_Number_of_Samples}\")\n",
    "    \n",
    "    # Get number of Correct Predictions\n",
    "    Number_of_Correct_Predictions = dataset[dataset[column_name_prediction]==dataset[column_name_label]].shape[0]\n",
    "    print(f\"Number of Correct Predictions {column_name_prediction}: {Number_of_Correct_Predictions}\")\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    accuracy = Number_of_Correct_Predictions / Total_Number_of_Samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY TWITTER DATA:\n",
      "\n",
      "Total Number of Samples: 4209\n",
      "Number of Correct Predictions VADER_class: 758\n",
      "Accuracy of VADER on Twitter Samples: 0.18009028272748873 meaning 18.01%\n",
      "\n",
      "Total Number of Samples: 4209\n",
      "Number of Correct Predictions BERT_class_precomputed: 2672\n",
      "Accuracy of BERT on Twitter Samples: 0.6348301259206462 meaning 63.48%\n",
      "\n",
      "Total Number of Samples: 4209\n",
      "Number of Correct Predictions GoogleNLP_class: 2825\n",
      "Accuracy of Google NLP on Twitter Samples: 0.6711808030411024 meaning 67.12%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACY TWITTER DATA:\\n\")\n",
    "\n",
    "accuracy_VADER_on_twitter = calculateAccuracy(twitter_data, \"label\", \"VADER_class\")\n",
    "print(f\"Accuracy of VADER on Twitter Samples: {accuracy_VADER_on_twitter} meaning {accuracy_VADER_on_twitter*100:.2f}%\\n\")\n",
    "\n",
    "accuracy_BERT_on_twitter = calculateAccuracy(twitter_data, \"label\", \"BERT_class_precomputed\")\n",
    "print(f\"Accuracy of BERT on Twitter Samples: {accuracy_BERT_on_twitter} meaning {accuracy_BERT_on_twitter*100:.2f}%\\n\")\n",
    "\n",
    "accuracy_GoogleNLP_on_twitter = calculateAccuracy(twitter_data, \"label\", \"GoogleNLP_class\")\n",
    "print(f\"Accuracy of Google NLP on Twitter Samples: {accuracy_GoogleNLP_on_twitter} meaning {accuracy_GoogleNLP_on_twitter*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY YOUTUBE DATA:\n",
      "\n",
      "Total Number of Samples: 3293\n",
      "Number of Correct Predictions VADER_class: 1374\n",
      "Accuracy of VADER on YouTube Samples: 0.41724870938354086 meaning 41.72%\n",
      "\n",
      "Total Number of Samples: 3293\n",
      "Number of Correct Predictions BERT_class_precomputed: 2448\n",
      "Accuracy of BERT on YouTube Samples: 0.7433950804737322 meaning 74.34%\n",
      "\n",
      "Total Number of Samples: 3293\n",
      "Number of Correct Predictions GoogleNLP_class: 2172\n",
      "Accuracy of Google NLP on YouTube Samples: 0.6595809292438506 meaning 65.96%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACY YOUTUBE DATA:\\n\")\n",
    "\n",
    "accuracy_VADER_on_youtube = calculateAccuracy(youtube_data, \"label\", \"VADER_class\")\n",
    "print(f\"Accuracy of VADER on YouTube Samples: {accuracy_VADER_on_youtube} meaning {accuracy_VADER_on_youtube*100:.2f}%\\n\")\n",
    "\n",
    "accuracy_BERT_on_youtube = calculateAccuracy(youtube_data, \"label\", \"BERT_class_precomputed\")\n",
    "print(f\"Accuracy of BERT on YouTube Samples: {accuracy_BERT_on_youtube} meaning {accuracy_BERT_on_youtube*100:.2f}%\\n\")\n",
    "\n",
    "accuracy_GoogleNLP_on_youtube = calculateAccuracy(youtube_data, \"label\", \"GoogleNLP_class\")\n",
    "print(f\"Accuracy of Google NLP on YouTube Samples: {accuracy_GoogleNLP_on_youtube} meaning {accuracy_GoogleNLP_on_youtube*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next calculate the precision of the `\"Positive\"` class for the Twitter and Youtube data.\n",
    "This is calculated as follows:\n",
    "$\n",
    "\\begin{align}\n",
    "    precision = \\frac{TP}{TP + FP}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class `\"Positive\"`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive (TP):** Observations belonging to the considered class are predicted as the correct class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Calculate True Positive (TP)\n",
    "def calculateTP(dataset: pd.DataFrame, column_name_label: str, column_name_prediction: str, class_value: str) -> int:\n",
    "    TP = dataset[(dataset[column_name_label] == class_value) & (dataset[column_name_prediction] == class_value)].shape[0]\n",
    "    return TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive (FP):** Observations not belonging to the considered class are incorrectly predicted as the considered class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Calculate False Positive (FP)\n",
    "def calculateFP(dataset: pd.DataFrame, column_name_label: str, column_name_prediction: str, class_value: str) -> int:\n",
    "    FP = dataset[(dataset[column_name_label] != class_value) & (dataset[column_name_prediction] == class_value)].shape[0]\n",
    "    return FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "    Precision = \\frac{TP}{TP + FP}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Calculate Precision\n",
    "def calculatePrecision(dataset: pd.DataFrame, column_name_label: str, column_name_prediction: str, class_value: str) -> float:\n",
    "    TP = calculateTP(dataset, column_name_label, column_name_prediction, class_value)\n",
    "    print(f\"True Positive of {column_name_prediction} considering Class \\\"{class_value}\\\" = {TP}\")\n",
    "    FP = calculateFP(dataset, column_name_label, column_name_prediction, class_value)\n",
    "    print(f\"False Positive of {column_name_prediction} considering Class \\\"{class_value}\\\" = {FP}\")\n",
    "    precision = TP / (TP + FP)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION TWITTER DATA (Class = \"Positive\"):\n",
      "\n",
      "True Positive of VADER_class considering Class \"Positive\" = 427\n",
      "False Positive of VADER_class considering Class \"Positive\" = 774\n",
      "Precision VADER on Twitter Samples: 0.3555370524562864 meaning 35.55%\n",
      "\n",
      "True Positive of BERT_class_precomputed considering Class \"Positive\" = 537\n",
      "False Positive of BERT_class_precomputed considering Class \"Positive\" = 964\n",
      "Precision BERT on Twitter Samples: 0.357761492338441 meaning 35.78%\n",
      "\n",
      "True Positive of GoogleNLP_class considering Class \"Positive\" = 328\n",
      "False Positive of GoogleNLP_class considering Class \"Positive\" = 651\n",
      "Precision Google NLP on Twitter Samples: 0.3350357507660878 meaning 33.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision on Twitter Data for class \"Positive\"\n",
    "print(\"PRECISION TWITTER DATA (Class = \\\"Positive\\\"):\\n\")\n",
    "\n",
    "precision_VADER_on_twitter_data = calculatePrecision(twitter_data, \"label\", \"VADER_class\", \"Positive\")\n",
    "print(f\"Precision VADER on Twitter Samples: {precision_VADER_on_twitter_data} meaning {precision_VADER_on_twitter_data*100:.2f}%\\n\")\n",
    "\n",
    "precision_BERT_on_twitter_data = calculatePrecision(twitter_data, \"label\", \"BERT_class_precomputed\", \"Positive\")\n",
    "print(f\"Precision BERT on Twitter Samples: {precision_BERT_on_twitter_data} meaning {precision_BERT_on_twitter_data*100:.2f}%\\n\")\n",
    "\n",
    "precision_GoogleNLP_on_twitter_data = calculatePrecision(twitter_data, \"label\", \"GoogleNLP_class\", \"Positive\")\n",
    "print(f\"Precision Google NLP on Twitter Samples: {precision_GoogleNLP_on_twitter_data} meaning {precision_GoogleNLP_on_twitter_data*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION YOUTUBE DATA (Class = \"Positive\"):\n",
      "\n",
      "True Positive of VADER_class considering Class \"Positive\" = 912\n",
      "False Positive of VADER_class considering Class \"Positive\" = 354\n",
      "Precision VADER on YouTube Samples: 0.7203791469194313 meaning 72.04%\n",
      "\n",
      "True Positive of BERT_class_precomputed considering Class \"Positive\" = 1202\n",
      "False Positive of BERT_class_precomputed considering Class \"Positive\" = 372\n",
      "Precision BERT on YouTube Samples: 0.7636594663278272 meaning 76.37%\n",
      "\n",
      "True Positive of GoogleNLP_class considering Class \"Positive\" = 914\n",
      "False Positive of GoogleNLP_class considering Class \"Positive\" = 270\n",
      "Precision Google NLP on YouTube Samples: 0.7719594594594594 meaning 77.20%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision on YouTube Data for class \"Positive\"\n",
    "print(\"PRECISION YOUTUBE DATA (Class = \\\"Positive\\\"):\\n\")\n",
    "\n",
    "precision_VADER_on_youtube_data = calculatePrecision(youtube_data, \"label\", \"VADER_class\", \"Positive\")\n",
    "print(f\"Precision VADER on YouTube Samples: {precision_VADER_on_youtube_data} meaning {precision_VADER_on_youtube_data*100:.2f}%\\n\")\n",
    "\n",
    "precision_BERT_on_youtube_data = calculatePrecision(youtube_data, \"label\", \"BERT_class_precomputed\", \"Positive\")\n",
    "print(f\"Precision BERT on YouTube Samples: {precision_BERT_on_youtube_data} meaning {precision_BERT_on_youtube_data*100:.2f}%\\n\")\n",
    "\n",
    "precision_GoogleNLP_on_youtube_data = calculatePrecision(youtube_data, \"label\", \"GoogleNLP_class\", \"Positive\")\n",
    "print(f\"Precision Google NLP on YouTube Samples: {precision_GoogleNLP_on_youtube_data} meaning {precision_GoogleNLP_on_youtube_data*100:.2f}%\\n\") \t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the recall score. This is done by:\n",
    "$\n",
    "\\begin{align}\n",
    "    recall = \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class `\"Positive\"`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Negative (FN):** Observations belonging to the considered class are incorrectly predicted as not belonging to the considered class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Calculate False Negative (FN)\n",
    "def calculateFN(dataset: pd.DataFrame, column_name_label: str, column_name_prediction: str, class_value: str) -> int:\n",
    "    FN = dataset[(dataset[column_name_label] == class_value) & (dataset[column_name_prediction] != class_value)].shape[0]\n",
    "    return FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "    Recall = \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Calculate Recall\n",
    "def calculateRecall(dataset: pd.DataFrame, column_name_label: str, column_name_prediction: str, class_value: str) -> float:\n",
    "    TP = calculateTP(dataset, column_name_label, column_name_prediction, class_value)\n",
    "    # print(f\"True Positive of {column_name_prediction} considering Class \\\"{class_value}\\\" = {TP}\")\n",
    "    FN = calculateFN(dataset, column_name_label, column_name_prediction, class_value)\n",
    "    print(f\"False Negative of {column_name_prediction} considering Class \\\"{class_value}\\\" = {FN}\")\n",
    "    recall = TP / (TP + FN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECALL TWITTER DATA (Class = \"Positive\"):\n",
      "\n",
      "False Negative of VADER_class considering Class \"Positive\" = 160\n",
      "Recall (class \"Positive\") VADER on Twitter Samples: 0.727427597955707 meaning 72.74%\n",
      "\n",
      "False Negative of BERT_class_precomputed considering Class \"Positive\" = 50\n",
      "Recall (class \"Positive\") BERT on Twitter Samples: 0.9148211243611585 meaning 91.48%\n",
      "\n",
      "False Negative of GoogleNLP_class considering Class \"Positive\" = 259\n",
      "Recall (class \"Positive\") Google NLP on Twitter Samples: 0.5587734241908007 meaning 55.88%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall on twitter data for class \"Positive\"\n",
    "print(\"RECALL TWITTER DATA (Class = \\\"Positive\\\"):\\n\")\n",
    "\n",
    "recall_VADER_on_twitter_data = calculateRecall(twitter_data, \"label\", \"VADER_class\", \"Positive\")\n",
    "print(f\"Recall (class \\\"Positive\\\") VADER on Twitter Samples: {recall_VADER_on_twitter_data} meaning {recall_VADER_on_twitter_data*100:.2f}%\\n\")\n",
    "\n",
    "recall_BERT_on_twitter_data = calculateRecall(twitter_data, \"label\", \"BERT_class_precomputed\", \"Positive\")\n",
    "print(f\"Recall (class \\\"Positive\\\") BERT on Twitter Samples: {recall_BERT_on_twitter_data} meaning {recall_BERT_on_twitter_data*100:.2f}%\\n\")\n",
    "\n",
    "recall_GoogleNLP_on_twitter_data = calculateRecall(twitter_data, \"label\", \"GoogleNLP_class\", \"Positive\")\n",
    "print(f\"Recall (class \\\"Positive\\\") Google NLP on Twitter Samples: {recall_GoogleNLP_on_twitter_data} meaning {recall_GoogleNLP_on_twitter_data*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECALL YOUTUBE DATA (Class = \"Positive\"):\n",
      "\n",
      "False Negative of VADER_class considering Class \"Positive\" = 401\n",
      "Recall (class \"Positive\") VADER on YouTube Samples: 0.6945925361766946 meaning 69.46%\n",
      "\n",
      "False Negative of BERT_class_precomputed considering Class \"Positive\" = 111\n",
      "Recall (class \"Positive\") BERT on YouTube Samples: 0.9154607768469155 meaning 91.55%\n",
      "\n",
      "False Negative of GoogleNLP_class considering Class \"Positive\" = 399\n",
      "Recall (class \"Positive\") Google NLP on YouTube Samples: 0.6961157654226962 meaning 69.61%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall on youtube data for class \"Positive\"\n",
    "print(\"RECALL YOUTUBE DATA (Class = \\\"Positive\\\"):\\n\")\n",
    "\n",
    "recall_VADER_on_youtube_data = calculateRecall(youtube_data, \"label\", \"VADER_class\", \"Positive\")\n",
    "print(f\"Recall (class \\\"Positive\\\") VADER on YouTube Samples: {recall_VADER_on_youtube_data} meaning {recall_VADER_on_youtube_data*100:.2f}%\\n\")\n",
    "\n",
    "recall_BERT_on_youtube_data = calculateRecall(youtube_data, \"label\", \"BERT_class_precomputed\", \"Positive\")\n",
    "print(f\"Recall (class \\\"Positive\\\") BERT on YouTube Samples: {recall_BERT_on_youtube_data} meaning {recall_BERT_on_youtube_data*100:.2f}%\\n\")\n",
    "\n",
    "recall_GoogleNLP_on_youtube_data = calculateRecall(youtube_data, \"label\", \"GoogleNLP_class\", \"Positive\")\n",
    "print(f\"Recall (class \\\"Positive\\\") Google NLP on YouTube Samples: {recall_GoogleNLP_on_youtube_data} meaning {recall_GoogleNLP_on_youtube_data*100:.2f}%\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Recall and the Precision score now also for the negative class. The Precision is calculated as:\n",
    "$\n",
    "\\begin{align}\n",
    "    precision = \\frac{TP}{TP + FP}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class `\"Negative\"`*\n",
    "\n",
    "And the Recall is calculated as:\n",
    "$\n",
    "\\begin{align}\n",
    "    recall = \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class `\"Negative\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION & RECALL on TWITTER DATA (Class = \"Negative\"):\n",
      "\n",
      "True Positive of VADER_class considering Class \"Negative\" = 331\n",
      "False Positive of VADER_class considering Class \"Negative\" = 2677\n",
      "Precision VADER on Twitter Samples: 0.11003989361702128 meaning 11.00%\n",
      "False Negative of VADER_class considering Class \"Negative\" = 50\n",
      "Recall (class \"Positive\") VADER on Twitter Samples: 0.868766404199475 meaning 86.88%\n",
      "\n",
      "True Positive of BERT_class_precomputed considering Class \"Negative\" = 312\n",
      "False Positive of BERT_class_precomputed considering Class \"Negative\" = 504\n",
      "Precision BERT on Twitter Samples: 0.38235294117647056 meaning 38.24%\n",
      "False Negative of BERT_class_precomputed considering Class \"Negative\" = 69\n",
      "Recall (class \"Positive\") BERT on Twitter Samples: 0.8188976377952756 meaning 81.89%\n",
      "\n",
      "True Positive of GoogleNLP_class considering Class \"Negative\" = 128\n",
      "False Positive of GoogleNLP_class considering Class \"Negative\" = 249\n",
      "Precision Google NLP on Twitter Samples: 0.3395225464190981 meaning 33.95%\n",
      "False Negative of GoogleNLP_class considering Class \"Negative\" = 253\n",
      "Recall (class \"Positive\") Google NLP on Twitter Samples: 0.3359580052493438 meaning 33.60%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision and Recall on Twitter Data for Class \"Negative\"\n",
    "print(\"PRECISION & RECALL on TWITTER DATA (Class = \\\"Negative\\\"):\\n\")\n",
    "\n",
    "neg_precision_VADER_on_twitter_data = calculatePrecision(twitter_data, \"label\", \"VADER_class\", \"Negative\")\n",
    "print(f\"Precision VADER on Twitter Samples: {neg_precision_VADER_on_twitter_data} meaning {neg_precision_VADER_on_twitter_data*100:.2f}%\")\n",
    "neg_recall_VADER_on_twitter_data = calculateRecall(twitter_data, \"label\", \"VADER_class\", \"Negative\")\n",
    "print(f\"Recall (class \\\"Positive\\\") VADER on Twitter Samples: {neg_recall_VADER_on_twitter_data} meaning {neg_recall_VADER_on_twitter_data*100:.2f}%\\n\")\n",
    "\n",
    "neg_precision_BERT_on_twitter_data = calculatePrecision(twitter_data, \"label\", \"BERT_class_precomputed\", \"Negative\")\n",
    "print(f\"Precision BERT on Twitter Samples: {neg_precision_BERT_on_twitter_data} meaning {neg_precision_BERT_on_twitter_data*100:.2f}%\")\n",
    "neg_recall_BERT_on_twitter_data = calculateRecall(twitter_data, \"label\", \"BERT_class_precomputed\", \"Negative\")\n",
    "print(f\"Recall (class \\\"Positive\\\") BERT on Twitter Samples: {neg_recall_BERT_on_twitter_data} meaning {neg_recall_BERT_on_twitter_data*100:.2f}%\\n\")\n",
    "\n",
    "neg_precision_GoogleNLP_on_twitter_data = calculatePrecision(twitter_data, \"label\", \"GoogleNLP_class\", \"Negative\")\n",
    "print(f\"Precision Google NLP on Twitter Samples: {neg_precision_GoogleNLP_on_twitter_data} meaning {neg_precision_GoogleNLP_on_twitter_data*100:.2f}%\")\n",
    "neg_recall_GoogleNLP_on_twitter_data = calculateRecall(twitter_data, \"label\", \"GoogleNLP_class\", \"Negative\")\n",
    "print(f\"Recall (class \\\"Positive\\\") Google NLP on Twitter Samples: {neg_recall_GoogleNLP_on_twitter_data} meaning {neg_recall_GoogleNLP_on_twitter_data*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION & RECALL on YOUTUBE DATA (Class = \"Negative\"):\n",
      "\n",
      "True Positive of VADER_class considering Class \"Negative\" = 462\n",
      "False Positive of VADER_class considering Class \"Negative\" = 1565\n",
      "Precision VADER on YouTube Samples: 0.227923038973853 meaning 22.79%\n",
      "False Negative of VADER_class considering Class \"Negative\" = 74\n",
      "Recall (class \"Positive\") VADER on YouTube Samples: 0.8619402985074627 meaning 86.19%\n",
      "\n",
      "True Positive of BERT_class_precomputed considering Class \"Negative\" = 436\n",
      "False Positive of BERT_class_precomputed considering Class \"Negative\" = 324\n",
      "Precision BERT on YouTube Samples: 0.5736842105263158 meaning 57.37%\n",
      "False Negative of BERT_class_precomputed considering Class \"Negative\" = 100\n",
      "Recall (class \"Positive\") BERT on YouTube Samples: 0.8134328358208955 meaning 81.34%\n",
      "\n",
      "True Positive of GoogleNLP_class considering Class \"Negative\" = 214\n",
      "False Positive of GoogleNLP_class considering Class \"Negative\" = 164\n",
      "Precision Google NLP on YouTube Samples: 0.5661375661375662 meaning 56.61%\n",
      "False Negative of GoogleNLP_class considering Class \"Negative\" = 322\n",
      "Recall (class \"Positive\") Google NLP on YouTube Samples: 0.39925373134328357 meaning 39.93%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision and Recall on YouTube Data for Class \"Negative\"\n",
    "print(\"PRECISION & RECALL on YOUTUBE DATA (Class = \\\"Negative\\\"):\\n\")\n",
    "\n",
    "neg_precision_VADER_on_youtube_data = calculatePrecision(youtube_data, \"label\", \"VADER_class\", \"Negative\")\n",
    "print(f\"Precision VADER on YouTube Samples: {neg_precision_VADER_on_youtube_data} meaning {neg_precision_VADER_on_youtube_data*100:.2f}%\")\n",
    "neg_recall_VADER_on_youtube_data = calculateRecall(youtube_data, \"label\", \"VADER_class\", \"Negative\")\n",
    "print(f\"Recall (class \\\"Positive\\\") VADER on YouTube Samples: {neg_recall_VADER_on_youtube_data} meaning {neg_recall_VADER_on_youtube_data*100:.2f}%\\n\")\n",
    "\n",
    "neg_precision_BERT_on_youtube_data = calculatePrecision(youtube_data, \"label\", \"BERT_class_precomputed\", \"Negative\")\n",
    "print(f\"Precision BERT on YouTube Samples: {neg_precision_BERT_on_youtube_data} meaning {neg_precision_BERT_on_youtube_data*100:.2f}%\")\n",
    "neg_recall_BERT_on_youtube_data = calculateRecall(youtube_data, \"label\", \"BERT_class_precomputed\", \"Negative\")\n",
    "print(f\"Recall (class \\\"Positive\\\") BERT on YouTube Samples: {neg_recall_BERT_on_youtube_data} meaning {neg_recall_BERT_on_youtube_data*100:.2f}%\\n\")\n",
    "\n",
    "neg_precision_GoogleNLP_on_youtube_data = calculatePrecision(youtube_data, \"label\", \"GoogleNLP_class\", \"Negative\")\n",
    "print(f\"Precision Google NLP on YouTube Samples: {neg_precision_GoogleNLP_on_youtube_data} meaning {neg_precision_GoogleNLP_on_youtube_data*100:.2f}%\") \n",
    "neg_recall_GoogleNLP_on_youtube_data = calculateRecall(youtube_data, \"label\", \"GoogleNLP_class\", \"Negative\")\n",
    "print(f\"Recall (class \\\"Positive\\\") Google NLP on YouTube Samples: {neg_recall_GoogleNLP_on_youtube_data} meaning {neg_recall_GoogleNLP_on_youtube_data*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To learn more\n",
    "1. What was the best performing method for Youtube? Did that fit your expectations?\n",
    "2. What was the best performing method for Twitter? Did that fit your expectations?\n",
    "4. Do you observe any differences between prediction of positive and negative sentiment? What is the role of the imbalance between postive and negative classes in the calculation of accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Calculation                    Class                DATA                 VADER                BERT                 GFoogle NLP         \n",
      "1     Precision Class                Positive             Twitter              0.36                 0.36                 0.34                \n",
      "2     Precision Class                Positive             YouTube              0.72                 0.76                 0.77                \n",
      "3     Precision Class                Negative             Twitter              0.11                 0.38                 0.34                \n",
      "4     Precision Class                Negative             YouTube              0.23                 0.57                 0.57                \n",
      "5     Recall Class                   Positive             Twitter              0.73                 0.91                 0.56                \n",
      "6     Recall Class                   Positive             YouTube              0.69                 0.92                 0.7                 \n",
      "7     Recall Class                   Negative             Twitter              0.87                 0.82                 0.34                \n",
      "8     Recall Class                   Negative             YouTube              0.86                 0.81                 0.4                 \n"
     ]
    }
   ],
   "source": [
    "# Summarize DATA\n",
    "d = {1: [\"Precision Class\", \"Positive\", \"Twitter\", round(precision_VADER_on_twitter_data,2), round(precision_BERT_on_twitter_data,2), round(precision_GoogleNLP_on_twitter_data,2)],\n",
    "2: [\"Precision Class\", \"Positive\", \"YouTube\", round(precision_VADER_on_youtube_data,2), round(precision_BERT_on_youtube_data,2), round(precision_GoogleNLP_on_youtube_data,2)],\n",
    "3: [\"Precision Class\", \"Negative\", \"Twitter\", round(neg_precision_VADER_on_twitter_data,2), round(neg_precision_BERT_on_twitter_data,2), round(neg_precision_GoogleNLP_on_twitter_data,2)],\n",
    "4: [\"Precision Class\", \"Negative\", \"YouTube\", round(neg_precision_VADER_on_youtube_data,2), round(neg_precision_BERT_on_youtube_data,2), round(neg_precision_GoogleNLP_on_youtube_data,2)],\n",
    "5: [\"Recall Class\", \"Positive\", \"Twitter\", round(recall_VADER_on_twitter_data,2), round(recall_BERT_on_twitter_data,2), round(recall_GoogleNLP_on_twitter_data,2)],\n",
    "6: [\"Recall Class\", \"Positive\", \"YouTube\", round(recall_VADER_on_youtube_data,2), round(recall_BERT_on_youtube_data,2), round(recall_GoogleNLP_on_youtube_data,2)],\n",
    "7: [\"Recall Class\", \"Negative\", \"Twitter\", round(neg_recall_VADER_on_twitter_data,2), round(neg_recall_BERT_on_twitter_data,2), round(neg_recall_GoogleNLP_on_twitter_data,2)],\n",
    "8: [\"Recall Class\", \"Negative\", \"YouTube\", round(neg_recall_VADER_on_youtube_data,2), round(neg_recall_BERT_on_youtube_data,2), round(neg_recall_GoogleNLP_on_youtube_data,2)]\n",
    "}\n",
    "print (\"{:<5} {:<30} {:<20} {:<20} {:<20} {:<20} {:<20}\".format('', 'Calculation', 'Class', 'DATA', 'VADER','BERT','GFoogle NLP'))\n",
    "for k, v in d.items():\n",
    "    calc, class_value, data, vader, bert, google = v\n",
    "    print (\"{:<5} {:<30} {:<20} {:<20} {:<20} {:<20} {:<20}\".format(k, calc, class_value, data, vader, bert, google)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Evaluation Summary\n",
    "\n",
    "### 1. Best Performing Method for YouTube\n",
    "The best method was **BERT** with an accuracy of $74.34\\%$, strong precision ($76.37\\%$), and recall ($91.55\\%$) for \"Positive\" sentiment. This aligns with expectations, as BERT effectively captures nuanced language.\n",
    "\n",
    "### 2. Best Performing Method for Twitter\n",
    "The best method was **Google NLP** with an accuracy of $67.12\\%$. This partially fits expectations, but its lower precision and recall indicate it struggles with informal language compared to BERT.\n",
    "\n",
    "### 3. Differences Between Positive and Negative Sentiment\n",
    "\"Positive\" sentiment generally shows higher recall, especially for BERT, while \"Negative\" sentiment precision tends to be lower (e.g., Google NLP). VADER often overpredicts \"Positive,\" leading to lower precision.\n",
    "\n",
    "### Role of Imbalance\n",
    "Imbalance skews accuracy by favoring majority classes, inflating overall metrics while ignoring minority class performance. Metrics like precision and recall highlight these disparities, showing BERT's robustness to class imbalance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e2835a142a16ae115bce5fddf19f27ce13b17a4ab8ded638c88ab5ce5171d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
